How to apply concurrent mechanisms to game engine's game loop.
# Task decomposition
To take advantage of parallel computing, we have to decompose tasks to execute each of them i parallel. 
There are many ways to decompose, but most of them will be in 2 categories: *task parallelism* and *data parallelism*.
First one is when number of tasks have to be done and we opt to do them in parallel across multiple cores.
Data parallelism is when a single computation needs to be performed repeatedly on a large number of data elements. GPU is best example.
**Different ways to subdivide the work that is done by the game loop.**
# One Thread per Subsystem
Assign engine subsystems to separate threads.
For example, the rendering engine, the collision and physics simulation, the animation pipeline, and the audio engine could each be assigned to its own thread.
Main thread would handle the control and synchronization.
This is example of *task parallelism.*
**There are a number of problems with the simple approach of assigning each engine subsystem to its own thread.**
First of all, number of subsystems will not match cores.
Another problem is that each subsystem requires different amount of processing each frame.
Another issue is that some subsystems depend on data produced by others.
**For example, the rendering and audio subsystems cannot start doing their work for frame N until the animation, collision and physics systems have completed their work for frame N.**
# Scatter/Gather
Many tasks are data intensive. For example, we may need to process a large number of ray cast requests, blend together a large batch of animation poses, or calculate world-space matrices for every object in a massive interactive scene.
One way to take advantage of parallel computing hardware is to perform those tasks with divide-and-conquer approach.
![[Pasted image 20250327193928.png]]
This is *data parallelism.*
## Scatter/Gather in Game Loop
Given a dataset containing N data items that require processing, the master thread would divide the work into m batches, each containing roughly N/m elements. (The value of m would probably be determined based on the number of available cores in the system, although this may not be the case if, for example, we wish to leave some cores free for other work.)
Then, m workers would be spawned.
Each worker thread might update the data items in-place, or (usually better) it might produce output data into a separate preallocated buffer (one per worker thread).
Main thread might perform some useful things while waiting for result
## SIMD for Scatter/Gather
SIMD could be used in lieu of thread-based scatter/gather, but it would likely be used in conjunction with it (with each worker thread utilizing vectorization internally to perform its work)
## More efficient Scatter/Gather
Spawning threads is expensive.
We could mitigate the costs of spawning threads by using of a pool of prespawned threads.
Really, what we want is a general-purpose system for executing units of work concurrently, across the available cores on our target hardware.
# Job system
General-purpose system for executing arbitrary units of work, typically called jobs, across multiple cores.
The job system maintains a queue of submitted jobs, and it schedules those jobs across the available cores, either by submitting them to be executed by worker threads in a thread pool, or by some other means.
## Typical Job system interface
Looks similar to threading library API.
Usually there is a function to *kick* a job (spawning), a function that allows one job to wait for another and perhaps a way to terminate job early.
Job system has to provide spin locks of mutexes of some kind for performing critical concurrent operations in an *atomic manner*. It may also provide facilities for putting jobs to sleep and waking them back up, via condition variables or some similar mechanism.
![[Pasted image 20250327195136.png]]
To kick a job, we need to tell what job to perform and how to do it. This information is passed to `KickJob()` via small structure - job declaration.
```
namespace job {
	typedef void EntryPoint(uintptr_t param);
	enum class Priority
	{
		LOW, NORMAL, HIGH, CRITICAL
	};
	struct Counter ... ;
	Counter* AllocCounter();
	void FreeCounter(Counter* pCounter);
	struct Declaration
	{
		EntryPoint* entryPoint;
		uintptr_t param;
		Priority priority;
		Counter* counter;
	};
	void KickJob(const Declaration& decl);
	void KickJobs(int count, const Declaration aDecl[]);
	void WaitForCounter(Counter* _counter);
	void KickJobAndWait(const Declaration& decl);
	void KickJobsAndWait(int count, const Declaration aDecl[]);
}
```

## Simple job system based on thread pool
It’s a good idea to spawn one thread for each CPU that’s present in the target machine, and use affinity to lock each thread to one core.
Each worker sits in infinite loop processing other jobs or waiting for conditional variable for a job to become available.
Example of job worker thread
```
namespace job {
	void* JobWorkerThread(void*)
	{
		//keep on running jobs forever
		while(true)
		{
			Declaration declCopy;
			//wait for a job to become available
			pthread_mutex_lock(&g_mutex);
			while(!g_ready)
			{
				pthread_cond_wait(&g_jobCv, &g_mutex);
			}
			//copy declaration and release lock
			declCopy = GetNextJobFromQueue();
			pthread_mutex_unlock(&g_mutex);
			//run the job
			declCopy.m_pEntryPoint(declCopy.m_param);			
		}
	}
}
```
## Limitations
```
void NpcThinkJob(uintparam_t param)
{
	Npc* pNpc = reinterpret_cast<Npc*>(param);
	pNpc->StartThinking();
	pNpc->DoMoreUpdating();
	//now lets cast a ray to see
	//if we aim at something interesting
	RayCastHandle hRayCast = CastGunAimRay(pNpc);
	WaitForRayCast(hRayCast);
	pNpc->TryFireWeaponAtTarget(hRayCast);
}
```
It will not work.
Each job shares thread, so it shares stack.
In thread pool based system, job must run to completion.
It cannot go to sleep and wait for another job to finish.
## Job as Coroutines
Coroutines can yield to one another like this because the im plementation actually does swap the callstacks of the outgoing and incoming coroutines within the thread in which the coroutines are running. So unlike a purely thread-based job, a coroutine-based job could effectively “go to sleep” andallow other jobs to run while it waits for an operation such as a ray cast to complete.
## Job as Fibers
Fiber-based system starts by converting one thread into a fiber.
Thread will continue to run that fiber until it calls SwitchToFiber() to yield control to another fiber. Just as with coroutines, entire callstack is saved. Fibers can even migrate from one thread to another.
## Job Counters
Whenever a job is kicked, it can optionally be associated with a counter(provided to it via the job::Declaration). The act of kicking the job increments the counter, and when the job terminates the counter is decremented. Waiting for a batch of jobs, then, involves simply kicking them all off with the same counter, and then waiting until that counter reaches zero (which indicates that all jobs have completed their work.) Waiting until a counter reaches zero is much more efficient than polling the individual jobs, because the check can be made at the moment the counter is decremented.
## Synchronization
Jobs could use spin locks instead of OS mutexes. This approach works well as long as there’s not very much lock contention between threads, because in that case no job will ever busy-wait for every lock trying to obtain a lock.

Custom mutex for high-contention situation:
Such a mutex might start by busy-waiting when a lock can’t be obtained. If after a brief timeout the lock is still unavailable, the mutex could yield the coroutine or fiber to another waiting job, thereby putting the waiting job to sleep. Just as the kernel keeps track of all sleeping threads that are waiting on a mutex, our job system would need to keep track of all sleeping jobs so that it can wake them up when their mutexes free up.

**Let’s have a look at how the fiber-based job system used in the Naughty Dog engine works. When the system first boots up, the main thread converts itself to a fiber in order to enable fibers within the process as a whole. Next, job worker threads are spawned, one for each of the seven cores available to developersonthePS4. ThesethreadsareeachlockedtoonecoreviatheirCPU affinity settings, so we can think of these worker threads and their cores as being roughly synonymous (although in reality, other higher-priority threads do sometimes interrupt the worker threads for very brief periods during the frame). Fiber creation is slow on the PS4, so a pool of fibers is pre-spawned, along with memory blocks to serve as each fiber’s call stack. When jobs are kicked, their declarations are placed onto a queue. As cores/worker threads become free (as jobs terminate), new jobs are pulled from this queue and executed. A running job can also add more jobs to the job queue.
To execute a job, an unused fiber is pulled from the fiber pool, and the worker thread performs a SwitchToFiber() to start the job running. When a job returns from its entry point function or otherwise self-terminates, the job’s final act is to perform a SwitchToFiber() back to the job system itself. It thenselectsanotherjobfromthequeue,andtheprocessrepeatsadinfinitum. When a job waits on a counter, the job is put to sleep and its fiber (exe cution context) is placed on a wait list, along with the counter it is waiting for. When this counter hits zero, the job is woken back up so it can continue where it left off. Sleeping and waking jobs is again implemented by calling SwitchToFiber() between the job’s fiber and the job system’s management fibers on each core/worker thread.**